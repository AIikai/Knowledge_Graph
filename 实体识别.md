- 实体识别

  命名实体识别，简称NER，是自然语言处理中的一项基础任务，应用广泛。命名实体一般指的是文本中具有特定意义或者指代性强的实体，通常包括人名、地名、组织机构名、专有名词等，NER则是从非结构化的输入文本中抽取出上述实体，并且可以按照业务需求识别出更多类别的实体。

	- 1. 基于规则

		- a. 自定义词典识别

		  自定义词典
		  构建实体词典进行匹配，通常可采用Trie树/Hashtable、Double array trie、AC自动机，另外可借助分词和句法工具保证词典匹配出的结果与文本分词后的结果兼容。
		  
		  优势：速度快、精准识别，适合实体明确、数量级小的场景
		  劣势：词典构建和更新麻烦，且词典有限，无法发现新词

		- b. 规则模板

		  人工编写规则，一般由具有一定领域知识的专家手工构建。
		  
		  优势：小数据集上可以达到较高的准确率和召回率。
		  劣势：随着数据集增大，规则集的构建周期变长，后期维护成本较高，而且模式的可移植性差，缺乏语义泛化能力，召回率普遍较低。

	- 2. 基于统计模型

	  资料https://zhuanlan.zhihu.com/p/71190655

		- a. Hidden Markov Models (HMM)

		  https://github.com/lipengfei-558/hmm_ner_organization
		  未知参数：实体类型
		  优势：考虑了上下文信息
		  劣势：假设了可观测变量之间相互独立，限制观测变量是词语本身，限制了特征的选择，如词频、位置等。

		- b. Maximum Entropy Markov Models (MEMM)

		  http://www.ai.mit.edu/courses/6.891-nlp/READINGS/maxent.pdf
		  生成模型变为判别模型，只计算给定可观测变量下隐藏变量的概率。
		  优势：打破了HMM的观测独立假设
		  劣势：由于局部归一化，会造成标注偏差问题

		- c. Conditional Random Fields

		  http://www.ai.mit.edu/courses/6.891-nlp/ASSIGNMENT1/t11.1.pdf
		  https://github.com/bojone/crf/
		  将最大熵马尔可夫模型里面的条件概率转化为特征函数的形式，分解为两部分，转移特征和状态特征。
		  优势：打破了观测独立假设，克服了标注偏差
		  劣势：训练速度较慢

	- 3. 基于深度学习

		- a. 序列标注

		  各模型的性能比较（数据来自论文）
		    
		  
		  从目前已发表的论文来看，Bert_Flat和后面提到的Bert_MRC是NER任务中效果最佳的，Flat尚未开源，后面有机会将会对目前主流的和最新的NER算法/模型做一下实验对比和总结。
		   详见：https://zhuanlan.zhihu.com/p/142615620
		  
		  补充|Flat已开源（正在看）
		  https://github.com/LeeSureman/Flat-Lattice-Transformer

			- 字符级别

				- (Bi)LSTM + CRF

				  https://arxiv.org/pdf/1603.01360.pdf
				    
				  
				  NER任务中比较主流的模型，包含三层结构，Embedding层（主要有词向量、字向量和一些额外特征）、双向LSTM层和最后的CRF层，如图所示。

				- Bert + (Bi)LSTM + CRF

				  https://github.com/EOA-AILab/NER-Chinese
				  使用Bert模型替换了原网络中的word2vec部分，从而构成Embedding层，同样使用双向LSTM层以及CRF层做序列预测。

			- 融合词汇信息

			  由于中文分词存在误差，基于字符的模型效果通常好于基于词汇（经过分词）的方法。但基于字符的NER没有利用词汇信息，而词汇边界对于实体边界通常起着至关重要的作用，所以说引入词汇信息，对中文NER任务很有必要。
			  
			  基于词汇增强的中文NER主要分为两条主线：
			  a. Dynamic Architecture：设计一个动态框架，能够兼容词汇输入
			  b. Adaptive Embedding：基于词汇信息，构建自适应Embedding
			  
			  ACL2020中的两篇论文FLAT和Simple-Lexicon分别对应于Dynamic Architecture和Adaptive Embedding，这两种方法相比于其他方法：
			  1）能够充分利用词汇信息，避免信息损失；
			  2）FLAT不去设计或改变原生编码结构，设计巧妙的位置向量就融合了词汇信息；Simple-Lexicon对于词汇信息的引入更加简单有效，采取静态加权的方法可以提前离线计算。

				- Dynamic Architecture

				  Dynamic Architecture范式通常需要设计相应结构以融入词汇信息。

					- Lattice LSTM

					  Lattice LSTM  2018
					  https://arxiv.org/pdf/1805.02023.pdf
					  基于词汇增强方法的中文NER的开篇之作，提出了一种Lattice结构以融合词汇信息。
					   
					  
					  Lattice是一个有向无环图，词汇的开始和结束字符决定了其位置。Lattice LSTM结构则是融合了词汇信息到原生的LSTM中，如图。
					    
					  
					  Lattice LSTM引入了一个word cell结构，对于当前的字符，融合以该字符结束的所有word信息。如‘桥’融合了‘大桥’和‘长江大桥’。
					  
					  优势：引入词汇信息，提升了NER性能
					  劣势：计算性能低下，不能batch并行化，Batch size只能为1；信息损失，每个字符只能获取以它为结尾的词汇信息，对于其之前的词汇信息没有持续记忆。如对于‘大’，其无法获得‘inside’的‘长江大桥’信息。另外由于RNN特性，采取Bi LSTM时其前向和后向的词汇信息不能共享；可迁移性差，只适配于LSTM，不具备向其他网络迁移的特性。

					- LR-CNN

					  LR -CNN：CNN-Based Chinese NER with Lexicon Rethinking  2019
					  https://pdfs.semanticscholar.org/1698/d96c6fffee9ec969e07a58bab62cb4836614.pdf
					  
					  针对Lattice LSTM采取RNN结构，导致其不能充分利用GPU进行并行化，以及RNN仅仅依靠前一步的信息输入，而非全局信息，无法有效处理词汇信息冲突的问题，LR -CNN提出：
					  
					  a. Lexicon-Based CNNs：采取CNN对字符特征进行编码，感受野大小为2提取bi-gram特征，堆叠多层获得multi-gram特征，同时采用注意力机制融入词汇信息
					  
					  b. Refining Networks with Lexicon Rethinking：对于词汇信息冲突的问题，LR-CNN采取rethinking机制增加feedback layer来调整词汇信息的权值，即将高层特征作为输入通过注意力模块调节每一层词汇特征分布。如图，高层特征得到的‘广州市’和‘长隆’会降低‘市长’在输出特征中的权重分布。最终对每一个字符位置提取对应的调整词汇信息分布后的multi-gram特征，输入CRF层解码。
					    
					  
					  优势：相比Lattice LSTM加速三倍多
					  劣势：计算复杂，不具备可迁移性

					- CGN

					  CGN:Leverage Lexical Knowledge for Chinese Named Entity Recognition via Collaborative Graph Network  2019
					  https://www.aclweb.org/anthology/D19-1396.pdf
					  
					  针对Lattice LSTM存在信息损失，尤其是无法获得‘inside’的词汇信息问题，提出了CGN --基于协作的图网络。不再详述。

					- LGN

					  LGN:A Lexicon-Based Graph Neural Network for Chinese NER  2019
					  https://www.aclweb.org/anthology/D19-1096.pdf
					  
					  针对Lattice LSTM使用RNN结构仅仅依靠前一步的信息输入，而不是全局信息，从而无法解决词汇冲突的问题，提出LGN。如下图所示，将每一个字符作为节点，匹配到的词汇信息构成边，通过图结构实现局部信息的聚合（Multi- Head Attention），并增加全局节点进行全局信息融入。

					- FLAT

					  FLAT: Chinese NER Using Flat-Lattice Transformer  2020  https://arxiv.org/pdf/2004.11795.pdf
					  
					  众所周知，Transformer采取全连接的自注意力机制可以很好地捕捉长距离依赖，由于自注意力机制对位置是无偏的，因此Transformer引入位置向量来保持位置信息。受到位置向量表征的启发，FLAT设计了一种巧妙的position encoding来融合Lattice 结构，如上图，对于每一个字符和词汇都构建两个head position encoding和tail position encoding，重构了原有的Lattice结构。因此，FLAT可以直接建模字符与所有匹配的词汇信息间的交互，例如，‘药’可以匹配到词汇‘人和药店’和‘药店’。
					    
					  
					  因此，我们可以将Lattice结构展平，将其从一个有向无环图展平为一个平面的Flat-Lattice Transformer结构，由多个span构成：每个字符的head和tail是相同的，每个词汇的head和tail是跳跃的。Flat结构如下图所示。

				- Adaptive Embedding

				  Adaptive Embedding范式仅在Embedding层对于词汇信息进行自适应，后面通常接入LSTM+CRF或其他通用网络，其与模型无关，具备可迁移性。

					- WC-LSTM

					  WC -LSTM: An Encoding Strategy Based Word-Character LSTM for Chinese NER Lattice LSTM  2019
					  https://pdfs.semanticscholar.org/43d7/4cd04fb22bbe61d650861766528e369e08cc.pdf?_ga=2.158312058.1142019791.1590478401-1756505226.1584453795
					    
					  
					  针对Lattice LSTM中每个字符只能获取以它为结尾的词汇数量是动态的、不固定的，从而导致不能Batch并行化的问题，WC -LSTM采取词汇编码策略，将每个字符为结尾的词汇信息进行固定编码表示，即每一个字符引入的词汇表征是静态的、固定的，如果没有对应的词汇则用<PAD>代替，从而实现Batch并行化。
					  
					  优势：相对于Lattice LSTM，可以并行化
					  劣势：信息损失，无法获得‘inside’的词汇信息，建模能力有限，效率低

					- Multi-digraph

					  Multi-digraph: A Neural Multi-digraph Model for Chinese NER with Gazetteers 2019 
					  https://www.aclweb.org/anthology/P19-1141.pdf
					  
					  不同于其他论文引入词汇信息是基于一个词表（由Lattice LSTM提供，没有实体标签，通常需要提取对应的word embedding），本文引入词汇信息的方式是利用实体词典（Gazetteers，含实体类型标签）。图结构，不再详述。

					- Simple-Lexicon

					  Simple-Lexicon：Simplify the Usage of Lexicon in Chinese NER  2020
					  https://arxiv.org/pdf/1908.05969.pdf
					  
					  本文提出一种在Embedding层简单利用词汇的方法，对比了三种不同的融合词汇的方式。
					  a. Softword
					  b. Extend Softword
					  c. Soft-lexicon
					    
					  
					  最佳表现是Soft-lexicon，它对当前字符，依次获取BMES对应所有词汇集合，然后再进行编码表示。

			- 融合字形信息

				- Glyce

				  文章：Glyph-vectors for Chinese Character Representations
				  https://arxiv.org/abs/1901.10125
				  https://github.com/ShannonAI/glyce  香侬官方开源
				  
				  Glyce，一种汉字字形的表征向量，将汉字当作图像，使用CNN去获取语义表征：
				  a. 使用历时的汉字字形（如金文、隶书、篆书等）和不同的书写风格（如行书、草书等）来丰富字符图像的象形信息，从而更全面地捕捉汉字的语义特征。
				  b. 提出Tianzige-CNN（田字格）架构，专门为中文象形字符建模而设计。
				  c. 添加图像分类的辅助损失，通过多任务学习增强模型的泛化能力。
				    
				  为了将字形信息和传统的词向量和字向量结合起来，香侬针对词级别的任务和字级别的任务，提出了两种Glyce模型结构。模型的结构总览见上图。在字级别的任务中，可以首先通过上述方法得到一个Glyph Embedding，然后和对应的Char Embedding结合起来，具体可以使用concat，highway或者全连接，然后送入下游任务。在词级别的任务中，首先有一个Word Embedding，对这个词而言，分别对其中的每一个字进行字级别的embedding操作，把所有的字级别的embedding用max-pooling结合起来，然后再和这个Word Embedding结合，送入下游任务。
				    
				    
				  
				  上面两图分别是Glyce与Bert的结合，以及将Glyce-Bert应用于不同任务时的情况。个人尚未验证效果，但训练很耗时，不适合快速落地。

		- b. 指针网络

		  指针网络通常应用于神经机器阅读理解，转化为2个n元分类预测头指针和尾指针，但这种方法只能抽取单一片段；对于实体识别，同一类型实体可能会存在多个，应转化为n个2元分类预测头指针和尾指针。

			- 堆叠式指针网络

			  简介：构造多类别的堆叠式指针网络，每一层指针网络对应一种实体类别。（了解后再补充...）

			- MRC QA + 单层指针网络

			  简介：构建query问题（即转化为机器阅读理解QA问题），问题即指代所要抽取的实体类型，同时引入了先验语义知识，从而能够在小数据集下、迁移学习下表现更好。
			    
			  
			  文章：A Unified MRC Framework for Named Entity Recognition
			  https://arxiv.org/pdf/1910.11476.pdf
			  https://github.com/qiufengyuyi/sequence_tagging 大神复现
			  https://github.com/ShannonAI/mrc-for-flat-nested-ner 香侬官方开源
			  	
			  基于MRC的命名实体识别任务，MRC分为三部分，问题、答案和文本，首先需要将序列标注数据变成这种适用于阅读理解任务的三元组，主要是对于要抽取的每一类实体，构造关于该类实体的Query，然后将需抽取实体的原始文本作为content，预测该类实体在content中的位置，一般是start_position和end_position，即实体的开始位置和结束位置。
			  
			  如下所示，{ 问题：“找出人名和虚构的人物形象”，答案：“彭真”，文本：“这次会议上彭真同志当选为全国人大常委会副委员长兼秘书长”}。
			    
			  
			  相比传统的序列标注做法，MRC方式做NER的好处在于引入了Query这个先验知识，比如对于PER这个类别，我们构造一个Query：找出人名和虚构的人物形象。模型通过attention机制，对于Query中的人名，人物形象学习到了person的关注信息，然后反哺到content中的实体信息捕捉中。
			    
			  模型以Bert为基础，输入时将问题和文本concat在一起，输出开始和结束位置，从而得到实体位置。
			  
			  Bert+MRC的优势：（官方说法）
			  1. 在训练数据较少的情况下优于Bert-Tagger；
			  2. 能够在统一框架下处理flat NER和嵌套NER任务；
			  3. 具有更好的zero-shot学习能力，可以预测训练集中不存在的标签，可应用在迁移学习上；
			  4. Query编码了有关要提取的实体类别的先验信息，有潜在能力消除相似类的歧义。

				- Bert + MRC

		- c. 片段排列 + 分类

		  序列标注和Span抽取的方法都是停留在token-level进行NER，间接去提取span-level的特征。而基于片段排列的方式，显示的提取所有可能的片段排列，由于选择的每一个片段都是独立的，因此可以直接提取span-level的特征去解决重叠实体问题。
		  
		  对于含T个token的文本，理论上共有 T(T+1)/2 种片段排列。如果文本过长，会产生大量的负样本，在实际中需要限制span长度并合理削减负样本。

